{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fce529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze step association\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "base_dir = \"c:/Users/darka/Desktop/Projets/Elexxion/\"\n",
    "input_dir = os.path.join(base_dir, \"association/raw\")\n",
    "valid_dir = os.path.join(base_dir, \"association/valid\")\n",
    "parquet_dir = os.path.join(base_dir, \"association/parquet/bronze\")\n",
    "expected_fields = 23\n",
    "delimiter = \";\"\n",
    "\n",
    "os.makedirs(valid_dir, exist_ok=True)\n",
    "os.makedirs(parquet_dir, exist_ok=True)\n",
    "\n",
    "csv_files = glob.glob(os.path.join(input_dir, \"*.csv\"))\n",
    "\n",
    "for input_path in csv_files:\n",
    "  filename = os.path.basename(input_path)\n",
    "  output_path = os.path.join(valid_dir, filename.replace(\".csv\", \"_valid.csv\"))\n",
    "\n",
    "  valid_lines = []\n",
    "  error_lines = []\n",
    "\n",
    "  with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line_number, line in enumerate(file, start=1):\n",
    "      fields = line.strip().split(delimiter)\n",
    "\n",
    "      if len(fields) == expected_fields:\n",
    "        valid_lines.append(line.strip())\n",
    "      else:\n",
    "        error_lines.append((line_number, line.strip()))\n",
    "\n",
    "  with open(output_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for line in valid_lines:\n",
    "      output_file.write(line + \"\\n\")\n",
    "\n",
    "valid_csv_files  = glob.glob(os.path.join(valid_dir, \"*.csv\"))\n",
    "\n",
    "for csv_path in valid_csv_files :\n",
    "  filename = os.path.basename(csv_path)\n",
    "  print(f\"Processing file: {filename}\")\n",
    "\n",
    "  match = re.search(r'rna_import_(\\d{8})_dpt_([0-9]{2}|[0-9]{3}|2A|2B|97[1-9]{1}[0-9]{1})', filename)\n",
    "  if match:\n",
    "    full_year = match.group(1)\n",
    "    year = full_year[:4]\n",
    "    dpt = match.group(2)\n",
    "\n",
    "    print(f\"Matched year: {year}, department: {dpt}\")\n",
    "    parquet_filename = f\"df_bronze_association_{year}_dpt_{dpt}.parquet\"\n",
    "  else:\n",
    "    print(f\"[‚ö†Ô∏è Ignored file (unknown pattern) : {filename}\")\n",
    "    continue\n",
    "\n",
    "  parquet_path = os.path.join(parquet_dir, parquet_filename)\n",
    "\n",
    "  df = pd.read_csv(csv_path, sep=delimiter, dtype=str)\n",
    "  df.to_parquet(parquet_path, index=False)\n",
    "  print(f\"Converted to parquet: {parquet_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62874204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver step association\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "bronze_dir = os.path.join(base_dir, \"association/parquet/bronze\")\n",
    "silver_dir = os.path.join(base_dir, \"association/parquet/silver\")\n",
    "\n",
    "os.makedirs(silver_dir, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(bronze_dir):\n",
    "  if filename.endswith(\".parquet\"):\n",
    "    file_path = os.path.join(bronze_dir, filename)\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    # Standardize column names\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Remove empty rows\n",
    "    df = df.dropna(how='all')\n",
    "\n",
    "    # Colums renaming\n",
    "    df.rename(columns={'adrs_codepostal': 'cp'}, inplace=True)\n",
    "    df.rename(columns={'date_publi': 'publication'}, inplace=True)\n",
    "    df.rename(columns={'libcom': 'commune'}, inplace=True)\n",
    "    df.rename(columns={'maj_time': 'maj'}, inplace=True)\n",
    "    df.rename(columns={'objet': 'resume'}, inplace=True)\n",
    "    df.rename(columns={'publication': 'creation'}, inplace=True)\n",
    "    df.rename(columns={'titre': 'nom'}, inplace=True)\n",
    "\n",
    "    # Colums deleting\n",
    "    df = df.drop(\n",
    "      columns=[\n",
    "        \"adr1\",\n",
    "        \"adr2\",\n",
    "        \"adr3\",\n",
    "        \"date_creat\",\n",
    "        \"dir_civilite\",\n",
    "        \"gestion\",\n",
    "        \"groupement\",\n",
    "        \"id_ex\",\n",
    "        \"nature\",\n",
    "        \"objet_social1\",\n",
    "        \"objet_social2\",\n",
    "        \"observation\",\n",
    "        \"position\",\n",
    "        \"rup_mi\",\n",
    "        \"siret\",\n",
    "        \"siteweb\"\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    # Save as Parquet with '_df_silver' prefix\n",
    "    silver_filename = filename.replace(\"df_bronze\", \"df_silver\")\n",
    "    silver_file_path = os.path.join(silver_dir, silver_filename)\n",
    "\n",
    "    df.to_parquet(silver_file_path, index=False)\n",
    "    print(f\"‚úÖ Silver files saved : {silver_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7a02bf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Aggregated file saved : df_gold_association_aggregated.parquet\n"
     ]
    }
   ],
   "source": [
    "# Gold step association\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_dir = \"c:/Users/darka/Desktop/Projets/Elexxion/\"\n",
    "silver_dir = os.path.join(base_dir, \"association/parquet/silver\")\n",
    "gold_dir = os.path.join(base_dir, \"association/parquet/gold\")\n",
    "os.makedirs(gold_dir, exist_ok=True)\n",
    "\n",
    "df_list = []\n",
    "\n",
    "departement_to_region = {\n",
    "  '01': '84', '02': '32', '03': '84', '04': '93', '05': '93', '06': '93',\n",
    "  '07': '84', '08': '44', '09': '76', '10': '44', '11': '76', '12': '76',\n",
    "  '13': '93', '14': '28', '15': '84', '16': '75', '17': '75', '18': '24',\n",
    "  '19': '75', '2A': '94', '2B': '94', '21': '27', '22': '53', '23': '75',\n",
    "  '24': '75', '25': '27', '26': '84', '27': '28', '28': '24', '29': '53',\n",
    "  '30': '76', '31': '76', '32': '76', '33': '75', '34': '76', '35': '53',\n",
    "  '36': '24', '37': '24', '38': '84', '39': '27', '40': '75', '41': '24',\n",
    "  '42': '84', '43': '84', '44': '52', '45': '24', '46': '76', '47': '75',\n",
    "  '48': '76', '49': '52', '50': '28', '51': '44', '52': '44', '53': '52',\n",
    "  '54': '44', '55': '44', '56': '53', '57': '44', '58': '27', '59': '32',\n",
    "  '60': '32', '61': '28', '62': '32', '63': '84', '64': '75', '65': '76',\n",
    "  '66': '76', '67': '44', '68': '44', '69': '84', '70': '27', '71': '27',\n",
    "  '72': '52', '73': '84', '74': '84', '75': '11', '76': '28', '77': '11',\n",
    "  '78': '11', '79': '75', '80': '32', '81': '76', '82': '76', '83': '93',\n",
    "  '84': '93', '85': '52', '86': '75', '87': '75', '88': '44', '89': '27',\n",
    "  '90': '27', '91': '11', '92': '11', '93': '11', '94': '11', '95': '11',\n",
    "  '971': '01', '972': '02', '973': '03', '974': '04', '976': '06'\n",
    "}\n",
    "valid_department = set(departement_to_region.keys())\n",
    "\n",
    "# Load silvers Parquet files\n",
    "for filename in os.listdir(silver_dir):\n",
    "  if filename.endswith(\".parquet\"):\n",
    "    file_path = os.path.join(silver_dir, filename)\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df_list.append(df)\n",
    "\n",
    "if df_list:\n",
    "  df_gold = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "  df_gold[\"cp\"] = df_gold[\"cp\"].astype(str).str.strip()\n",
    "  df_gold = df_gold[df_gold[\"cp\"].str.match(r\"^\\d{5}$\")]\n",
    "\n",
    "  df_gold[\"departement\"] = df_gold[\"cp\"].str[:2]\n",
    "  df_gold = df_gold[df_gold[\"departement\"].isin(valid_department)]\n",
    "\n",
    "  df_gold[\"annee\"] = pd.to_datetime(df_gold[\"maj\"], errors=\"coerce\").dt.year\n",
    "\n",
    "  df_agg = df_gold.groupby([\"departement\", \"annee\"]).size().reset_index(name=\"nombre_nouvelle_asso\")\n",
    "  df_agg = df_agg.sort_values(by=[\"departement\", \"annee\"])\n",
    "\n",
    "  df_agg[\"region\"] = df_agg[\"departement\"].map(departement_to_region)\n",
    "  df_agg[\"cumul_global\"] = df_agg.groupby(\"departement\")[\"nombre_nouvelle_asso\"].cumsum()\n",
    "\n",
    "  columns_order = [\"departement\", \"region\", \"annee\", \"nombre_nouvelle_asso\", \"cumul_global\"]\n",
    "  df_agg = df_agg[columns_order]\n",
    "\n",
    "  agg_filename = \"df_gold_association_aggregated.parquet\"\n",
    "  agg_file_path = os.path.join(gold_dir, agg_filename)\n",
    "  df_agg.to_parquet(agg_file_path, index=False)\n",
    "  print(f\"üìä Aggregated file saved : {agg_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
